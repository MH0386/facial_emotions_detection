{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787}],"dockerImageVersionId":30529,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mh0386/facial-emotions-detection?scriptVersionId=156333364\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#----------------------------------------#\n!pip install xgboost                     #\n!pip install --upgrade                   #\n#----------------------------------------#","metadata":{"execution":{"iopub.status.busy":"2023-12-24T15:22:02.344592Z","iopub.execute_input":"2023-12-24T15:22:02.344918Z","iopub.status.idle":"2023-12-24T15:22:11.086951Z","shell.execute_reply.started":"2023-12-24T15:22:02.344887Z","shell.execute_reply":"2023-12-24T15:22:11.085892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nprint(\"Num GPUs Available:\", (tf.config.experimental.list_physical_devices('TPU')))\nfrom keras.applications.xception import Xception\nimport bz2\nimport os\nimport xgboost as xgb\nimport requests\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import img_to_array , load_img\nfrom keras.models import load_model\nimport scipy\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input as p_input\nfrom keras.utils import to_categorical\nimport numpy as np\nimport cv2\nimport requests\nfrom keras.utils import plot_model\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.datasets import make_classification\nfrom deap import base, creator, tools, algorithms","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:31:21.501207100Z","start_time":"2023-08-26T15:31:21.440809700Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T15:35:19.44509Z","iopub.execute_input":"2023-12-24T15:35:19.445415Z","iopub.status.idle":"2023-12-24T15:35:19.451425Z","shell.execute_reply.started":"2023-12-24T15:35:19.445388Z","shell.execute_reply":"2023-12-24T15:35:19.450692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #!pip install requests\n# # import requests\n# # import bz2\n# import dlib\n\n# # # Download the shape predictor file compressed in bz2 format\n# # url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n# # response = requests.get(url)\n# # with open(\"shape_predictor_68_face_landmarks.dat.bz2\", \"wb\") as f:\n# #     f.write(response.content)\n\n# # # Decompress the downloaded bz2 file\n# # with open(\"shape_predictor_68_face_landmarks.dat.bz2\", \"rb\") as f:\n# #     compressed_data = f.read()\n# #\n# # decompressed_data = bz2.decompress(compressed_data)\n# #\n# # # Save the decompressed data to a file\n# # with open(\"shape_predictor_68_face_landmarks.dat\", \"wb\") as f:\n# #     f.write(decompressed_data)\n# #\n# # print(\"Shape predictor file downloaded and decompressed.\")\n\n# # Now you can use the dlib detector and predictor\n# datFile = \"shape_predictor_68_face_landmarks.dat\"\n# detector = dlib.get_frontal_face_detector()\n# predictor = dlib.shape_predictor(datFile)\n","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:24:10.378549700Z","start_time":"2023-08-26T15:24:08.556866Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T14:06:26.345243Z","iopub.status.idle":"2023-12-24T14:06:26.345735Z","shell.execute_reply.started":"2023-12-24T14:06:26.345485Z","shell.execute_reply":"2023-12-24T14:06:26.345508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#reading and Augmenting data and preprocess it for VGG16\n# def custom_preprocess_input(img):\n#     processed_img = p_input(img)\n#     return processed_img","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:22:11.732268300Z","start_time":"2023-08-26T15:22:11.714657300Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T14:06:26.782726Z","iopub.execute_input":"2023-12-24T14:06:26.783478Z","iopub.status.idle":"2023-12-24T14:06:26.787244Z","shell.execute_reply.started":"2023-12-24T14:06:26.783445Z","shell.execute_reply":"2023-12-24T14:06:26.786331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load images and labels\ngeneral_path = \"/kaggle/input/fer2013/\"\npath_train_dataset = general_path + \"train/\"\npath_validation_dataset = general_path + \"test/\"\npicture_size=224 #input of VGG16 should be 224 AND input for mibile network should be 227\nbatch_size=256\nnumber_of_classes=7\n# Data Augmentation using ImageDataGenerator\ndatagen = ImageDataGenerator(\n    preprocessing_function=p_input,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n)\n\n# Load train and test datasets\ntrain_data_set = datagen.flow_from_directory(\n    directory=path_train_dataset,\n    target_size=(picture_size, picture_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=True\n)\n\ntest_data_set = datagen.flow_from_directory(\n    directory=path_validation_dataset,\n    target_size=(picture_size, picture_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=True\n)","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:22:16.657435900Z","start_time":"2023-08-26T15:22:13.928278500Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T15:35:28.213029Z","iopub.execute_input":"2023-12-24T15:35:28.213383Z","iopub.status.idle":"2023-12-24T15:35:35.490857Z","shell.execute_reply.started":"2023-12-24T15:35:28.213352Z","shell.execute_reply":"2023-12-24T15:35:35.489968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data_set.filenames)","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:22:17.782173300Z","start_time":"2023-08-26T15:22:17.751621300Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T15:35:53.455133Z","iopub.execute_input":"2023-12-24T15:35:53.455886Z","iopub.status.idle":"2023-12-24T15:35:53.460722Z","shell.execute_reply.started":"2023-12-24T15:35:53.455852Z","shell.execute_reply":"2023-12-24T15:35:53.46002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"expression = \"happy\"\nplt.figure(figsize=(12, 12))\n\nfor i in range(1, 13):\n    plt.subplot(4, 4, i)\n    img_path = os.path.join(path_train_dataset, expression,\n                            os.listdir(os.path.join(path_train_dataset, expression))[i])\n    img = load_img(img_path, target_size=(48, 48))\n    plt.imshow(img)\n    plt.title(f\"Image {i}\")\n    plt.axis(\"off\")\n    plt.title(expression)\nplt.show()\n\nlabels = []\nimages=[]\n\n\n","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:22:21.298005700Z","start_time":"2023-08-26T15:22:19.893201700Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T15:35:56.80947Z","iopub.execute_input":"2023-12-24T15:35:56.809784Z","iopub.status.idle":"2023-12-24T15:35:57.355141Z","shell.execute_reply.started":"2023-12-24T15:35:56.809755Z","shell.execute_reply":"2023-12-24T15:35:57.354323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"expression = \"angry\"\nplt.figure(figsize=(10, 10))\n\nfor i in range(1, 10):\n    plt.subplot(3, 3, i)\n    img_path = os.path.join(path_train_dataset, expression,\n                            os.listdir(os.path.join(path_train_dataset, expression))[i])\n    img = load_img(img_path, target_size=(48, 48))\n    plt.imshow(img)\n    plt.title(f\"Image {i}\")\n    plt.axis(\"off\")\n    plt.title(expression)\nplt.show()\n","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:22:23.504962600Z","start_time":"2023-08-26T15:22:22.099909100Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T15:36:01.674545Z","iopub.execute_input":"2023-12-24T15:36:01.674874Z","iopub.status.idle":"2023-12-24T15:36:02.344062Z","shell.execute_reply.started":"2023-12-24T15:36:01.674846Z","shell.execute_reply":"2023-12-24T15:36:02.343151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_set.next()[0].shape #get shape of each patch ","metadata":{"ExecuteTime":{"end_time":"2023-08-26T16:20:53.037363300Z","start_time":"2023-08-26T16:20:50.714488500Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T15:36:19.474994Z","iopub.execute_input":"2023-12-24T15:36:19.475365Z","iopub.status.idle":"2023-12-24T15:36:23.273874Z","shell.execute_reply.started":"2023-12-24T15:36:19.475332Z","shell.execute_reply":"2023-12-24T15:36:23.273154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now we will try to use exception Feature extraction architicture ","metadata":{}},{"cell_type":"code","source":"from keras.utils import plot_model\nnon_trainable_model = Xception(weights='imagenet', include_top=False, input_shape=(picture_size, picture_size, 3))\nfor layer in non_trainable_model.layers:\n    layer.trainable = False\n# output_features_of_freezed_model = non_trainable_model.predict(train_data_set)\nnon_trainable_model.summary()\nplot_model(non_trainable_model, to_file='Xception_base_model.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T15:36:28.183945Z","iopub.execute_input":"2023-12-24T15:36:28.184264Z","iopub.status.idle":"2023-12-24T15:36:29.628055Z","shell.execute_reply.started":"2023-12-24T15:36:28.184236Z","shell.execute_reply":"2023-12-24T15:36:29.618048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Know we want to access extracted features of data to feed into XG_boost with genatic algorithm optimizer**","metadata":{}},{"cell_type":"code","source":"train_data=non_trainable_model.predict(train_data_set)\ntest_data = train_data=non_trainable_model.predict(test_data_set)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T16:02:26.747554Z","iopub.execute_input":"2023-12-24T16:02:26.747931Z","iopub.status.idle":"2023-12-24T16:14:35.279061Z","shell.execute_reply.started":"2023-12-24T16:02:26.747898Z","shell.execute_reply":"2023-12-24T16:14:35.278023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape ","metadata":{"execution":{"iopub.status.busy":"2023-12-24T16:33:03.561551Z","iopub.execute_input":"2023-12-24T16:33:03.561917Z","iopub.status.idle":"2023-12-24T16:33:03.567126Z","shell.execute_reply.started":"2023-12-24T16:33:03.561886Z","shell.execute_reply":"2023-12-24T16:33:03.56645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Assuming you have a dataset X and corresponding labels y\n# For illustration purposes, I'm using a synthetic dataset\n\n# X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, random_state=42)\n\n\n# Define the XGBoost fitness function for multi-class classification\n\n# we use compination of f1_scores_ and recall to give high penality into false negative \ndef xgboost_fitness(hyperparameters):\n    params = {\n        'objective': 'multi:softmax',\n        'num_class': len(np.unique(y)),  # Number of classes\n        'eval_metric': 'mlogloss',\n        'eta': hyperparameters[0],  # Learning rate\n        'max_depth': int(hyperparameters[1]),  # Maximum depth of a tree\n        'subsample': hyperparameters[2],  # Subsample ratio of the training instances\n        'colsample_bytree': hyperparameters[3],  # Subsample ratio of columns when constructing each tree\n        'min_child_weight': int(hyperparameters[4]),  # Minimum sum of instance weight (hessian) needed in a child\n    }\n\n    # Train XGBoost model\n    model = xgb.XGBClassifier(**params)\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # Evaluate metrics\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Combine metrics into a single fitness score\n    fitness = f1 * (1 + 0.1 * (1 - recall))\n\n    return fitness,\n\n# Define genetic algorithm parameters\nNUM_GENERATIONS = 10\nPOPULATION_SIZE = 10\nCXPB, MUTPB = 0.7, 0.2\n\n# Define the hyperparameter space\nHYPERPARAMETER_SPACE = [\n    ('eta', 0.01, 1.0),  # Learning rate\n    ('max_depth', 1, 10),  # Maximum depth of a tree\n    ('subsample', 0.1, 1.0),  # Subsample ratio of the training instances\n    ('colsample_bytree', 0.1, 1.0),  # Subsample ratio of columns when constructing each tree\n    ('min_child_weight', 1, 10),  # Minimum sum of instance weight (hessian) needed in a child\n]\n\n# Create a fitness function for the genetic algorithm\ncreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\n\n\n\n#--------------------------------------------------------------------------------------------------------------------\n#this is the steps of genatic algorithm \n#first initialize individual at the start of first cycle of generation \n#second initialize the population of last initialized  individuales\n#check the fitness value\n#check make crossover and mutation  using gaussian distibutionwith CXPB, MUTPB probabilities of crossover and mutation \n# select the high fitness value and make a new generation cycle and so on until you reach the optimal hyperparams \n#---------------------------------------------------------------------------------------------------------------------\n#toolbox is a container for using genatic algorithm \n#---------------------------------------------------------------------------------------------------------------------\n\n\n\ntoolbox = base.Toolbox()\ntoolbox.register(\"attr_float\", np.random.uniform, low=0, high=1)\ntoolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.attr_float, ), n=len(HYPERPARAMETER_SPACE))\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\ntoolbox.register(\"evaluate\", xgboost_fitness)\ntoolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\ntoolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.2)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n\n# Main genetic algorithm loop\npopulation = toolbox.population(n=POPULATION_SIZE)\n\nfor gen in range(NUM_GENERATIONS):\n    offspring = algorithms.varAnd(population, toolbox, cxpb=CXPB, mutpb=MUTPB)\n    fits = toolbox.map(toolbox.evaluate, offspring)\n    for ind, fit in zip(offspring, fits):\n        ind.fitness.values = fit\n    population = toolbox.select(offspring, k=len(population))\n\n# Get the best individual (best set of hyperparameters)\nbest_individual = tools.selBest(population, k=1)[0]\nbest_hyperparameters = [param[1].low + value * (param[1].high - param[1].low) for param, value in zip(HYPERPARAMETER_SPACE, best_individual)]\n\nprint(\"Best Hyperparameters:\", best_hyperparameters)\n\n","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:22:33.310134400Z","start_time":"2023-08-26T15:22:33.298764Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T15:50:04.97891Z","iopub.execute_input":"2023-12-24T15:50:04.97917Z","iopub.status.idle":"2023-12-24T15:50:05.128675Z","shell.execute_reply.started":"2023-12-24T15:50:04.979145Z","shell.execute_reply":"2023-12-24T15:50:05.127632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"VGG16 Model","metadata":{"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:22:38.060888600Z","start_time":"2023-08-26T15:22:37.040967600Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T13:49:13.311554Z","iopub.status.idle":"2023-12-24T13:49:13.312031Z","shell.execute_reply.started":"2023-12-24T13:49:13.311762Z","shell.execute_reply":"2023-12-24T13:49:13.311782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model=tf.keras.Sequential(\n# [\n#         non_trainable_model, #the extracted features from VGG16 will be used as input of dense layers which is new model (\"we applied transfer learning \")\n#         tf.keras.layers.Flatten(),\n#         tf.keras.layers.Dense(512, activation=tf.nn.relu),\n#         tf.keras.layers.Dense(256, activation=tf.nn.relu),\n#         tf.keras.layers.Dense(128, activation=tf.nn.relu),\n#         tf.keras.layers.Dense(number_of_classes, activation=tf.nn.softmax)\n#     ]\n# )\n# Model.summary()\n# from keras.utils import plot_model\n# Model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n#               loss='categorical_crossentropy',\n#               metrics=['accuracy'])\n\n","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:23:23.345690100Z","start_time":"2023-08-26T15:23:23.005880800Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T13:49:13.312941Z","iopub.status.idle":"2023-12-24T13:49:13.313305Z","shell.execute_reply.started":"2023-12-24T13:49:13.313142Z","shell.execute_reply":"2023-12-24T13:49:13.313158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model.fit_generator(\n#     train_data_set,\n#     steps_per_epoch=train_data_set.n // train_data_set.batch_size,\n#     epochs=10,\n#     validation_data=test_data_set,\n#     validation_steps=test_data_set.n // test_data_set.batch_size\n    \n# )","metadata":{"ExecuteTime":{"end_time":"2023-08-26T15:23:26.999432100Z","start_time":"2023-08-26T15:23:25.701049500Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T13:49:13.314436Z","iopub.status.idle":"2023-12-24T13:49:13.314755Z","shell.execute_reply.started":"2023-12-24T13:49:13.314594Z","shell.execute_reply":"2023-12-24T13:49:13.31461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # plot_model(Model, to_file='model_architecture.png', show_shapes=True)\n# # Save the model\n# Model.save('face_emotion_model_VGG16.h5')\n\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T13:49:13.315958Z","iopub.status.idle":"2023-12-24T13:49:13.316294Z","shell.execute_reply.started":"2023-12-24T13:49:13.31614Z","shell.execute_reply":"2023-12-24T13:49:13.316155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Plot the accuracy and loss curves\n# fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n# axes[0].plot(history.history['accuracy'], label='Train accuracy')\n# axes[0].plot(history.history['val_accuracy'], label='Validation accuracy')\n# axes[0].set_xlabel('Epochs')\n# axes[0].legend()\n# axes[1].plot(history.history['loss'], label='Train loss')\n# axes[1].plot(history.history['val_loss'], label='Validation loss')\n# axes[1].set_xlabel('Epochs')\n# axes[1].legend()\n# plt.show()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T13:46:16.392551Z","iopub.status.idle":"2023-12-24T13:46:16.392854Z","shell.execute_reply.started":"2023-12-24T13:46:16.392705Z","shell.execute_reply":"2023-12-24T13:46:16.392719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import cv2\n# import dlib\n# import numpy as np\n# from keras.models import load_model\n# from keras.preprocessing.image import img_to_array\n\n\n# model=load_model(\".h5\")\n# # Load face detector from dlib\n# detector = dlib.get_frontal_face_detector()\n\n# # Define the emotion labels\n# emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n\n# # Initialize the webcam\n# cap = cv2.VideoCapture(0)\n\n# while True:\n#     # Read a frame from the webcam\n#     ret, frame = cap.read()\n\n#     if not ret:\n#         break\n\n#     # Convert the frame to grayscale\n#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n#     # Detect faces in the frame\n#     faces = detector(gray)\n\n#     for face in faces:\n#         x, y, w, h = face.left(), face.top(), face.width(), face.height()\n\n#         # Extract the face region\n#         face_roi = gray[y:y+h, x:x+w]\n#         face_roi = cv2.resize(face_roi, (224, 224))\n#         face_roi = p_input(face_roi)\n#         face_roi = img_to_array(face_roi)\n#         face_roi = np.expand_dims(face_roi, axis=0)\n\n#         # Predict the emotion using the model\n#         emotion_probabilities = model.predict(face_roi)[0]\n#         predicted_emotion = emotion_labels[np.argmax(emotion_probabilities)]\n\n#         # Draw a rectangle around the detected face and label the emotion\n#         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n#         # lanmarks=predictor(face)\n#         # for point in range(68):\n#         #     x=lanmarks.part(point).x\n#         #     y=lanmarks.part(point).y\n#         #     cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n#         cv2.putText(frame, predicted_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n#     # Display the frame\n#     cv2.imshow(\"Emotion Detection\", frame)\n\n#     # Exit when the 'q' key is pressed\n#     if cv2.waitKey(1) & 0xFF == ord('q'):\n#         break\n\n# # Release the webcam and close all windows\n# cap.release()\n# cv2.destroyAllWindows()\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T13:49:17.578725Z","iopub.execute_input":"2023-12-24T13:49:17.579374Z","iopub.status.idle":"2023-12-24T13:49:17.633677Z","shell.execute_reply.started":"2023-12-24T13:49:17.57934Z","shell.execute_reply":"2023-12-24T13:49:17.63246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"/","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T13:46:16.396013Z","iopub.status.idle":"2023-12-24T13:46:16.396321Z","shell.execute_reply.started":"2023-12-24T13:46:16.396169Z","shell.execute_reply":"2023-12-24T13:46:16.396184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"/","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-24T13:46:16.397532Z","iopub.status.idle":"2023-12-24T13:46:16.397872Z","shell.execute_reply.started":"2023-12-24T13:46:16.397692Z","shell.execute_reply":"2023-12-24T13:46:16.397713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}